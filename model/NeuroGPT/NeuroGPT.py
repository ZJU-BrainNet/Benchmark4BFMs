from torch import nn
import torch
from argparse import Namespace
from model.pre_cnn import ChannelConverter
from einops import rearrange

from model.NeuroGPT.decoder.make_decoder import make_decoder
from model.NeuroGPT.embedder.make import make_embedder
from model.NeuroGPT.decoder.unembedder import make_unembedder
from model.NeuroGPT.encoder.conformer_braindecode import EEGConformer
from model.NeuroGPT.model import Model
from model.model_config import ModelPathArgs


class NeuroGPT_Trainer:
    def __init__(self, args: Namespace):
        return

    @staticmethod
    def set_config(args: Namespace):
        args.ft_only_encoder = True
        args.training_style = 'decoding'    # CSM_causal, decoding
        args.filter_time_length = 25
        args.pool_time_length =75
        args.stride_avg_pool = 15
        args.final_dim = args.n_filters_time = 1080
        args.num_encoder_layers = 6
        args.num_hidden_layers = 6
        args.parcellation_dim = args.patch_len*22   # 1024
        args.embedding_dim = 1024
        args.num_hidden_layers_embedding_model = 1
        args.n_positions = 512
        args.num_attention_heads = 1
        args.intermediate_dim_factor = 4
        args.hidden_activation = 'gelu_new'
        args.num_hidden_layers_unembedding_model = 1
        args.chunk_len = args.patch_len
        if args.seq_len * 10 // 10 != args.seq_len:
            args.seq_len = int(args.seq_len * 10 // 10)
        
        return args

    @staticmethod
    def clsf_loss_func(args):
        if args.weights is None:
            ce_weight = [1.0 for _ in range(args.n_class)]
        else:
            ce_weight = args.weights
        print(f'CrossEntropy loss weight = {ce_weight} = {ce_weight[1]/ce_weight[0]:.2f}')
        return nn.CrossEntropyLoss(torch.tensor(ce_weight, dtype=torch.float32, device=torch.device(args.gpu_id)))


    @staticmethod
    def optimizer(args, model, clsf):
        return torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=00.1,
                          betas=(0.9, 0.999), eps=1e-8,)
        
    @staticmethod
    def scheduler(optimizer):
        def lr_lambda(current_step):
            if current_step < 400000 * 0.01:
                return float(current_step) / float(400000 * 0.01)
            return max(
                0.0, float(400000 - current_step) / float(400000 * (1.0 - 0.01))
            )
        return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)


class NeuroGPT(nn.Module):
    def __init__(self, args: Namespace):
        super(NeuroGPT, self).__init__()
        self.gpt = self.make_model(args)
        self.ch_cnn = ChannelConverter(in_channels=args.cnn_in_channels, out_channels=22)


    @staticmethod
    def make_model(args: Namespace):
        """Make model from model_config 
        (as generated by get_config()).
        """
        chann_coords = None
        
        encoder = EEGConformer(n_outputs=args.n_class, n_chans=22,    # fixed channel number
                               n_times=args.chunk_len, ch_pos=chann_coords, 
                               is_decoding_mode=args.ft_only_encoder, chunk_num=args.seq_len)
        #calculates the output dimension of the encoder, which is the output of transformer layer.
        args.parcellation_dim = ((args.chunk_len - args.filter_time_length + 1 - args.pool_time_length) // args.stride_avg_pool + 1) * args.n_filters_time


        embedder = make_embedder(
            training_style=args.training_style,
            architecture='GPT',
            in_dim=args.parcellation_dim, # flattened, channel x chunk length
            embed_dim=args.embedding_dim,
            num_hidden_layers=args.num_hidden_layers_embedding_model,
            dropout=0.1,
            n_positions=args.n_positions
        )
        decoder = make_decoder(
            architecture='GPT',
            num_hidden_layers=args.num_hidden_layers,
            embed_dim=args.embedding_dim,
            num_attention_heads=args.num_attention_heads,
            n_positions=args.n_positions,
            intermediate_dim_factor=args.intermediate_dim_factor,
            hidden_activation=args.hidden_activation,
            dropout=0.1
        )

        if args.embedding_dim != args.parcellation_dim:
            unembedder = make_unembedder(
                embed_dim=args.embedding_dim,
                num_hidden_layers=args.num_hidden_layers_unembedding_model,
                out_dim=args.parcellation_dim,
                dropout=0.1,
            )
        else:
            print("No Embedder and Unembedder!")
            unembedder = None

    
        model = Model(
            encoder=encoder,
            embedder=embedder,
            decoder=decoder,
            unembedder=unembedder,
        )

        if args.ft_only_encoder:
            model.switch_ft_mode(ft_encoder_only=True)

        if args.training_style == 'decoding':     # 'CSM_causal'
            model.switch_decoding_mode(
                is_decoding_mode=True,
                num_decoding_classes=args.n_class,
            )
        
        pretrained_path = ModelPathArgs.NeuroLM_path
        model.from_pretrained(pretrained_path)

        for name, param in model.encoder.named_parameters():
            if 'fc.' in name or 'embedder.' \
            or 'final_layer' in name:
                continue
            else:
                param.requires_grad = False

        for name, param in model.decoder.named_parameters():
            if 'pooler_layer' in name \
            or 'decoding_head' in name \
            or 'is_next_head' in name:
                continue
            else:
                param.requires_grad = False

        # if unembedder is not None:
        #     for param in model.unembedder.parameters():
        #         param.requires_grad = False

        return model
    
    
    def forward(self, x):
        return self.gpt(x)
    
    
    @staticmethod
    def forward_propagate(args, data_packet, model, clsf, loss_func=None):
        x, y = data_packet
        bsz, ch_num, N = x.shape
        if N % args.patch_len != 0:
            args.seq_len = int(N // args.patch_len)
            x = x[:, :, :args.seq_len*args.patch_len]
        x = x.reshape(bsz, ch_num, -1, args.patch_len)
        
        if ch_num != 22:
            if ch_num > 22:
                x = x[:, :22]
            else:
                x = x.reshape(bsz, ch_num, -1)
                if args.is_parallel:
                    x = model.module.ch_cnn(x)
                else:
                    x = model.ch_cnn(x)
        x = x.reshape(bsz, N // args.patch_len, 22, args.patch_len)
        
        batch = {}
        batch['inputs'] = x
        logit = model(batch)
        logit = logit['outputs']

        if args.run_mode != 'test':
            loss = loss_func(logit, y)
            return loss, logit, y
        else:
            return logit, y